['<div id="jobDescriptionText" class="jobsearch-jobDescriptionText"><div><p>Our culture lifts you up—there is no ego in the way. Our common purpose? We all want to win for our customers. We aim to always be evolving, dynamic, and ambitious. We believe in the power of genuine connections. Each employee is a part of what makes us unique on the market: agile, dedicated, problem solvers.\n</p><p></p><h2 class="jobSectionHeader"><b>Time Type:\n</b></h2>Regular\n<p></p><h2 class="jobSectionHeader"><b>Job Description :\n</b></h2><p><b>Job Summary:\n</b></p><p>This role is responsible for the extraction, transformation and load of data from our operational systems into our new Google Cloud environment. This position will be part of the Data &amp; Analytics team.\n</p><p></p><p><b>Responsibilities </b>:\n</p><ul><li><p>Assembling large, complex sets of data that meet non-functional and functional business requirements.\n</p></li><li><p>Identifying, designing and implementing internal process improvements optimizing data delivery, and automating manual processes.\n</p></li><li><p>Building required infrastructure for optimal extraction, transformation and loading of data from various data sources including MySQL, Oracle, flat files, CSV into GCP.\n</p></li><li><p>The process involves:\n</p><ul><li><p>Data modelling: working with the data analyst and GCP engineers to define the data requirements, the source of the data and the formats.\n</p></li><li><p>Data Architecture: defining, together with the GCP engineers, the architecture of the GCP databases.\n</p></li><li><p>Data pipeline development: this includes extraction, formatting, and uploading of the data.\n</p></li></ul></li></ul><p><b>Skills:\n</b></p><ul><li><p>Ability to build and optimize data sets, \'big data\' data pipelines and architectures.\n</p></li><li><p>Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvements.\n</p></li><li><p>Excellent analytic skills associated with working on unstructured datasets.\n</p></li><li><p>Ability to build automated processes that support data transformation, workload management, data structures, dependency and metadata using tools and scripting languages readily available on a variety of operating systems.\n</p></li></ul><p></p><p><b>Education &amp; Knowledge:\n</b></p><ul><li><p>Bachelor\'s degree in Information Technology, Computer Science or equivalent combination of training and/or experience.\n</p></li><li><p>Ability to analyze existing systems and software to understand current processes and designs.\n</p></li><li><p>Experience with UNIX/Linux environments.\n</p></li><li><p>Familiarity accessing APIs and consumer Interfaces that utilize XML, JSON, REST, SOAP, etc.\n</p></li><li><p>Scripting/programming: Powershell, PERL, Bash etc.\n</p></li><li><p>Functional knowledge of encryption technologies: SSL, TLS, SSH etc.\n</p></li><li><p>Knowledge of relational databases: MySQL/MariaDB, Oracle, MS SQL, Big Query.\n</p></li><li><p>Strong initiative to find ways to improve solutions, systems, and processes.\n</p></li><li><p>5-10 years of experience in data &amp; analytics.\n</p></li></ul><p></p><h2 class="jobSectionHeader"><b>Location :\n</b></h2>Montréal, QC\n<p></p><h2 class="jobSectionHeader"><b>Company :\n</b></h2>Cogeco Communications Inc.\n<p></p><ul><li><p>Being inclusive is simply welcoming you to be yourself!\n</p></li><li><p>Being inclusive also means fostering a climate of trust and respect, and encouraging diversity and equity for every candidate wanting to be part of the Cogeco team.\n</p></li><li><p>Being inclusive allows us to reduce barriers in the workplace and to take actions to enable accessibility for all!\n</p></li><li><p>Being inclusive is more than a word, it\'s our commitment to you!<br>\n</p></li></ul><p><br>\nIf you need any accommodations to apply or as part of the recruitment process, please contact us confidentially at inclusiondiversite@cogeco.com</p></div><p></p></div>']